{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "- LDA is a probabilistic model where each word is assigned to a topic and each document is assigned to a topic based on its content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/beridzeg45/guardian-environment-related-news\n",
      "License(s): other\n",
      "Downloading guardian-environment-related-news.zip to e:\\nlp\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/57.3M [00:00<?, ?B/s]\n",
      "  2%|▏         | 1.00M/57.3M [00:00<00:39, 1.49MB/s]\n",
      "  5%|▌         | 3.00M/57.3M [00:00<00:13, 4.22MB/s]\n",
      "  9%|▊         | 5.00M/57.3M [00:01<00:08, 6.28MB/s]\n",
      " 10%|█         | 6.00M/57.3M [00:01<00:09, 5.46MB/s]\n",
      " 12%|█▏        | 7.00M/57.3M [00:01<00:08, 6.12MB/s]\n",
      " 14%|█▍        | 8.00M/57.3M [00:01<00:10, 5.03MB/s]\n",
      " 16%|█▌        | 9.00M/57.3M [00:01<00:09, 5.32MB/s]\n",
      " 17%|█▋        | 10.0M/57.3M [00:02<00:09, 5.35MB/s]\n",
      " 19%|█▉        | 11.0M/57.3M [00:02<00:09, 5.34MB/s]\n",
      " 21%|██        | 12.0M/57.3M [00:02<00:08, 5.40MB/s]\n",
      " 23%|██▎       | 13.0M/57.3M [00:02<00:08, 5.41MB/s]\n",
      " 24%|██▍       | 14.0M/57.3M [00:02<00:08, 5.42MB/s]\n",
      " 26%|██▌       | 15.0M/57.3M [00:03<00:08, 5.40MB/s]\n",
      " 28%|██▊       | 16.0M/57.3M [00:03<00:07, 5.43MB/s]\n",
      " 30%|██▉       | 17.0M/57.3M [00:03<00:07, 5.40MB/s]\n",
      " 31%|███▏      | 18.0M/57.3M [00:03<00:08, 4.96MB/s]\n",
      " 33%|███▎      | 19.0M/57.3M [00:03<00:07, 5.57MB/s]\n",
      " 35%|███▍      | 20.0M/57.3M [00:04<00:07, 5.42MB/s]\n",
      " 38%|███▊      | 22.0M/57.3M [00:04<00:04, 7.73MB/s]\n",
      " 42%|████▏     | 24.0M/57.3M [00:04<00:03, 8.96MB/s]\n",
      " 45%|████▌     | 26.0M/57.3M [00:04<00:03, 9.83MB/s]\n",
      " 49%|████▉     | 28.0M/57.3M [00:04<00:02, 10.4MB/s]\n",
      " 52%|█████▏    | 30.0M/57.3M [00:05<00:03, 9.22MB/s]\n",
      " 58%|█████▊    | 33.0M/57.3M [00:05<00:02, 10.7MB/s]\n",
      " 61%|██████    | 35.0M/57.3M [00:05<00:01, 12.2MB/s]\n",
      " 65%|██████▍   | 37.0M/57.3M [00:05<00:01, 12.0MB/s]\n",
      " 68%|██████▊   | 39.0M/57.3M [00:05<00:01, 12.0MB/s]\n",
      " 72%|███████▏  | 41.0M/57.3M [00:05<00:01, 11.9MB/s]\n",
      " 75%|███████▌  | 43.0M/57.3M [00:06<00:01, 11.8MB/s]\n",
      " 78%|███████▊  | 45.0M/57.3M [00:06<00:01, 11.6MB/s]\n",
      " 82%|████████▏ | 47.0M/57.3M [00:06<00:00, 11.6MB/s]\n",
      " 85%|████████▌ | 49.0M/57.3M [00:06<00:00, 11.5MB/s]\n",
      " 89%|████████▉ | 51.0M/57.3M [00:06<00:00, 11.5MB/s]\n",
      " 92%|█████████▏| 53.0M/57.3M [00:06<00:00, 11.5MB/s]\n",
      " 96%|█████████▌| 55.0M/57.3M [00:07<00:00, 11.5MB/s]\n",
      " 99%|█████████▉| 57.0M/57.3M [00:07<00:00, 11.4MB/s]\n",
      "100%|██████████| 57.3M/57.3M [00:07<00:00, 8.15MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d beridzeg45/guardian-environment-related-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  guardian-environment-related-news.zip\n",
      "  inflating: guardian_environment_news.csv  \n"
     ]
    }
   ],
   "source": [
    "!unzip guardian-environment-related-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dasha\\AppData\\Local\\Temp\\ipykernel_27184\\2099715607.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text['index'] = data_text.index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Article Text    0\n",
       "index           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('guardian_environment_news.csv')\n",
    "data_text = data[['Article Text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "documents = documents.dropna()\n",
    "documents.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29691\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liz Truss will sign off on a push for more oil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It is an area so tranquil that the notion of b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Visits to parks, community gardens and other u...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I devised today’s nut roast for Oddbox, a veg ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‘Constant companions to our gardening’A peacoc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text  index\n",
       "0  Liz Truss will sign off on a push for more oil...      0\n",
       "1  It is an area so tranquil that the notion of b...      1\n",
       "2  Visits to parks, community gardens and other u...      2\n",
       "3  I devised today’s nut roast for Oddbox, a veg ...      3\n",
       "4  ‘Constant companions to our gardening’A peacoc...      4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(documents))\n",
    "documents.columns = ['article_text', 'index']\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- `Tokenization`\n",
    "- Remove `stopwords` and whose len < 3\n",
    "- words are `lemmatized` : third person -> first & verbs -> present\n",
    "- words are `stemmed` - words are reduces to their root form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `gensim` & `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dasha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Error loading corpora/wordnet: Package 'corpora/wordnet'\n",
      "[nltk_data]     not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('corpora/wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dasha\\AppData\\Roaming\\nltk_data\\corpora\\wordnet.zip\n",
      "Extraction successful!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = str(nltk.find('corpora/wordnet.zip'))\n",
    "print(zip_path)\n",
    "# Directory to extract to\n",
    "extract_to = 'C:/Users/dasha/AppData/Roaming/nltk_data/corpora/'\n",
    "\n",
    "# Check if the zip file exists\n",
    "if os.path.exists(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print('Extraction successful!')\n",
    "else:\n",
    "    print('Zip file not found!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      " ['The', 'US', 'is', 'set', 'to', 'impose', 'new', 'carbon', 'pollution', 'standards', 'upon', 'its', 'coal-', 'and', 'gas-fired', 'power', 'plants,', 'in', 'a', 'move']\n",
      "\n",
      "Tokenized document: \n",
      " ['impos', 'carbon', 'pollut', 'standard', 'coal', 'fire', 'power', 'plant', 'biden', 'administr', 'hail', 'major', 'step', 'confront', 'climat', 'crisi', 'rule', 'forward', 'environment', 'protect']\n"
     ]
    }
   ],
   "source": [
    "# checking the preprocessing on an random doc\n",
    "import random\n",
    "rand_idx = random.randint(0, len(documents))\n",
    "\n",
    "doc_sample = documents[documents['index'] == rand_idx].values[0][0]\n",
    "\n",
    "words = []\n",
    "for word in doc_sample.split():\n",
    "    words.append(word)\n",
    "print(\"Original document: \\n\", words[:20])\n",
    "print(\"\\nTokenized document: \\n\", preprocess(doc_sample)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 25.9 s\n",
      "Wall time: 2min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [truss, sign, push, drill, north, win, conserv...\n",
       "1    [area, tranquil, notion, bitter, disput, huge,...\n",
       "2    [visit, park, communiti, garden, urban, green,...\n",
       "3    [devis, today, roast, oddbox, outfit, support,...\n",
       "4    [constant, companion, garden, peacock, butterf...\n",
       "Name: article_text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processed_docs = documents['article_text'].map(preprocess)\n",
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words \n",
    "- keeping track of occurance of a word in the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accord - 0\n",
      "acquiesc - 1\n",
      "action - 2\n",
      "add - 3\n",
      "advis - 4\n",
      "alongsid - 5\n",
      "amid - 6\n",
      "astronom - 7\n",
      "averag - 8\n",
      "backdrop - 9\n",
      "begin - 10\n",
      "bill - 11\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs) # type: ignore\n",
    "\n",
    "for i, (k, v) in enumerate(dictionary.iteritems()):\n",
    "    print(v, \"-\", k)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2bow\n",
    "- here for each article_text we create a list containing the tuple -> (word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1), (8, 1), (14, 2), (17, 2), (25, 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[rand_idx][:5] # showing the first 5 of the article_text at rand_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- previewing the Bag Of Words for this random article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 3 ('add') appears 1 times\n",
      "Word 8 ('averag') appears 1 times\n",
      "Word 14 ('bring') appears 2 times\n",
      "Word 17 ('campaign') appears 2 times\n",
      "Word 25 ('compani') appears 1 times\n",
      "Word 26 ('conserv') appears 1 times\n",
      "Word 29 ('crisi') appears 1 times\n",
      "Word 31 ('daili') appears 2 times\n",
      "Word 43 ('email') appears 3 times\n",
      "Word 45 ('environment') appears 1 times\n",
      "Word 59 ('follow') appears 1 times\n",
      "Word 60 ('fossil') appears 4 times\n",
      "Word 63 ('fuel') appears 4 times\n",
      "Word 68 ('global') appears 2 times\n",
      "Word 70 ('guardian') appears 2 times\n",
      "Word 72 ('help') appears 1 times\n",
      "Word 82 ('issu') appears 1 times\n",
      "Word 91 ('make') appears 3 times\n",
      "Word 93 ('minist') appears 14 times\n",
      "Word 97 ('near') appears 1 times\n",
      "Word 105 ('plan') appears 3 times\n"
     ]
    }
   ],
   "source": [
    "bow_doc_rand_idx = bow_corpus[rand_idx]\n",
    "\n",
    "for i in range(len(bow_doc_rand_idx)):\n",
    "    print(f\"Word {bow_doc_rand_idx[i][0]} ('{dictionary[bow_doc_rand_idx[i][0]]}') appears {bow_doc_rand_idx[i][1]} times\")\n",
    "    # showing the first 20\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "- measuring the importance of each terms wrt to the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accord 0.021720469344865333\n",
      "acquiesc 0.13370066637362965\n",
      "action 0.022747440597037094\n",
      "add 0.02555825704747701\n",
      "advis 0.04687115007599941\n",
      "alongsid 0.05060409589593413\n",
      "amid 0.10154038427481077\n",
      "astronom 0.1066579402076301\n",
      "averag 0.03826728129436504\n",
      "backdrop 0.08911723157013195\n",
      "begin 0.02740707839979547\n",
      "bill 0.21957816352952608\n",
      "billion 0.046908926527445174\n",
      "brexit 0.05873605543784116\n",
      "bring 0.04997892807397803\n",
      "busi 0.07531999745073897\n",
      "call 0.03891221274467663\n",
      "campaign 0.027461320698540632\n",
      "cash 0.06501048142775517\n",
      "centuri 0.03701860482255549\n",
      "chief 0.034108586641713026\n",
      "choke 0.08291800622706784\n",
      "closur 0.14115708934988128\n",
      "coast 0.041013976278716446\n",
      "committe 0.04954980567719998\n",
      "compani 0.02360565663405371\n",
      "conserv 0.028331041491428476\n",
      "contin 0.06357514999565907\n",
      "countri 0.0150928292604786\n",
      "crisi 0.04547793320996474\n",
      "critic 0.03324952940011084\n",
      "daili 0.0351666233941124\n",
      "decad 0.024597704669070155\n",
      "depend 0.04234001409531421\n",
      "discuss 0.1320128215012223\n",
      "dismay 0.09238506911016088\n",
      "domest 0.051353683719497153\n",
      "doug 0.09514729044551777\n",
      "draw 0.04415413735475725\n",
      "drill 0.2458403172291083\n",
      "drive 0.03012167796518311\n",
      "effici 0.049256581259342874\n",
      "elect 0.04317254529020377\n",
      "email 0.027663375225131757\n",
      "energi 0.1441216930352441\n",
      "environment 0.019622539146331955\n",
      "europ 0.03781686185018082\n",
      "european 0.0416396194369463\n",
      "eventu 0.05483704699686832\n",
      "explain 0.04651967709923203\n",
      "explor 0.04914565180150622\n",
      "extra 0.053765634373688635\n",
      "facil 0.05341086768826764\n",
      "factor 0.0498918229658286\n",
      "faster 0.0549868946978596\n",
      "financi 0.04056467727667608\n",
      "firm 0.04363152666504476\n",
      "flaw 0.07852994432488763\n",
      "flow 0.050096451305675645\n",
      "follow 0.02429185270693703\n",
      "fossil 0.031929764296584236\n",
      "freez 0.06310800665554034\n",
      "frenzi 0.10016804734317154\n",
      "fuel 0.05299514708004405\n",
      "fund 0.02310032936180737\n",
      "geopolit 0.08982855876899067\n",
      "giant 0.04950639945057738\n",
      "gift 0.07744227275873236\n",
      "global 0.019712625728364963\n",
      "greenpeac 0.0586376238874731\n",
      "guardian 0.020932086729144598\n",
      "heavili 0.05853969990536581\n",
      "help 0.03466512762193656\n",
      "home 0.022394749023829243\n",
      "household 0.10001442281035239\n",
      "impact 0.0212682055192386\n",
      "import 0.02137040937033545\n",
      "improv 0.03542924076351779\n",
      "insul 0.06752386406341819\n",
      "intensifi 0.0714394384108401\n",
      "invas 0.0595571482972187\n",
      "involv 0.034656005690883014\n",
      "issu 0.023855590227434973\n",
      "jacob 0.09288926374927986\n",
      "kwarteng 0.09838081561911505\n",
      "kwasi 0.09826744085920451\n",
      "labour 0.04957589711599119\n",
      "lead 0.023871320837311055\n",
      "leadership 0.05457479427590709\n",
      "licenc 0.12781084588855737\n",
      "littl 0.05853278686431671\n",
      "make 0.021534690755271143\n",
      "meet 0.027573004137571126\n",
      "minist 0.027186290757232556\n",
      "mogg 0.10831030504063831\n",
      "moreth 0.037658220303540074\n",
      "mount 0.06319695396513049\n",
      "near 0.022881407179200824\n",
      "north 0.11147041095378984\n",
      "norway 0.06988134083975592\n",
      "opportun 0.039181346516348574\n",
      "parr 0.10755811895741127\n",
      "part 0.03469969333425943\n",
      "payer 0.11333106193829207\n",
      "pipelin 0.06652070326269519\n",
      "plan 0.05445414528473553\n",
      "polici 0.020818803673744515\n",
      "poorest 0.07571141638685076\n",
      "previous 0.0635378616028463\n",
      "price 0.036038020937494426\n",
      "produc 0.02697598131759207\n",
      "product 0.05641834532587666\n",
      "profit 0.04361880290183541\n",
      "project 0.02500854968269554\n",
      "promot 0.047015102132432145\n",
      "proper 0.05298954778080376\n",
      "propos 0.03532211407177932\n",
      "public 0.020184388453992806\n",
      "pump 0.054016874716531424\n",
      "push 0.03490356878149801\n",
      "putin 0.08719336701114558\n",
      "quarter 0.05013226501903343\n",
      "ramp 0.07463937355695613\n",
      "real 0.03662291469922074\n",
      "ree 0.09991980569221269\n",
      "regress 0.11098360622875272\n",
      "reli 0.047213874322750565\n",
      "report 0.05370553182472337\n",
      "rough 0.10951369374187067\n",
      "scientist 0.02827426101593958\n",
      "scrambl 0.08227420844209563\n",
      "secretari 0.043141510447735884\n",
      "secur 0.07576514401058386\n",
      "sell 0.03923175858704584\n",
      "short 0.035893510987705754\n",
      "sign 0.041003541328878776\n",
      "slash 0.07108498044415952\n",
      "soar 0.061041633891899295\n",
      "solar 0.04832148943048119\n",
      "standoff 0.11054703031469931\n",
      "storag 0.05644145934622829\n",
      "strap 0.09613145967223753\n",
      "sunak 0.07034328778748224\n",
      "suppli 0.07031694967072134\n",
      "support 0.021242740271579862\n",
      "take 0.015982796180785026\n",
      "tax 0.06727911272630827\n",
      "term 0.021441489487804635\n",
      "think 0.020849457781901143\n",
      "today 0.031017307135904486\n",
      "truss 0.5176190148244142\n",
      "twitter 0.040870361396204015\n",
      "typic 0.05569424708018807\n",
      "ukrain 0.06669140298175294\n",
      "unleash 0.08911723157013195\n",
      "vladimir 0.09280429619867475\n",
      "want 0.021630349511662237\n",
      "wide 0.04165681812420143\n",
      "win 0.06339411701477884\n",
      "wind 0.037326585500705105\n",
      "winter 0.08599879964703502\n",
      "yorkshir 0.06697210485934993\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "#     pprint(doc)\n",
    "    for i in range(len(doc)):\n",
    "        print(dictionary[doc[i][0]], doc[i][1]) # word : significance\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LDA with Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.94 s\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                       num_topics=10,\n",
    "                                       id2word=dictionary,\n",
    "                                       passes=2,\n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.015*\"energi\" + 0.011*\"emiss\" + 0.008*\"australia\" + 0.008*\"power\" + 0.007*\"coal\" + 0.007*\"carbon\" + 0.007*\"plan\" + 0.006*\"electr\" + 0.006*\"project\" + 0.006*\"industri\"\n",
      "Topic: 1 Word: 0.011*\"fish\" + 0.010*\"ocean\" + 0.009*\"research\" + 0.008*\"whale\" + 0.007*\"studi\" + 0.007*\"water\" + 0.006*\"scientist\" + 0.005*\"univers\" + 0.005*\"marin\" + 0.004*\"temperatur\"\n",
      "Topic: 2 Word: 0.012*\"speci\" + 0.010*\"farm\" + 0.009*\"anim\" + 0.008*\"protect\" + 0.007*\"farmer\" + 0.007*\"natur\" + 0.006*\"conserv\" + 0.006*\"wildlif\" + 0.006*\"land\" + 0.006*\"environ\"\n",
      "Topic: 3 Word: 0.013*\"countri\" + 0.010*\"global\" + 0.007*\"emiss\" + 0.005*\"meat\" + 0.005*\"nation\" + 0.005*\"report\" + 0.005*\"food\" + 0.004*\"crisi\" + 0.004*\"australia\" + 0.004*\"develop\"\n",
      "Topic: 4 Word: 0.016*\"plastic\" + 0.009*\"wast\" + 0.008*\"recycl\" + 0.007*\"compani\" + 0.005*\"product\" + 0.005*\"environment\" + 0.005*\"food\" + 0.004*\"work\" + 0.004*\"industri\" + 0.003*\"mine\"\n",
      "Topic: 5 Word: 0.005*\"tree\" + 0.005*\"photograph\" + 0.005*\"bird\" + 0.004*\"plant\" + 0.004*\"look\" + 0.003*\"natur\" + 0.003*\"leav\" + 0.003*\"work\" + 0.003*\"live\" + 0.003*\"know\"\n",
      "Topic: 6 Word: 0.025*\"water\" + 0.010*\"flood\" + 0.008*\"river\" + 0.006*\"area\" + 0.005*\"state\" + 0.005*\"home\" + 0.004*\"citi\" + 0.004*\"communiti\" + 0.004*\"south\" + 0.004*\"drought\"\n",
      "Topic: 7 Word: 0.015*\"pollut\" + 0.010*\"citi\" + 0.008*\"health\" + 0.006*\"london\" + 0.005*\"level\" + 0.005*\"emiss\" + 0.005*\"road\" + 0.004*\"public\" + 0.004*\"car\" + 0.004*\"transport\"\n",
      "Topic: 8 Word: 0.010*\"compani\" + 0.010*\"water\" + 0.008*\"parti\" + 0.007*\"shark\" + 0.005*\"polici\" + 0.005*\"environ\" + 0.004*\"newslett\" + 0.004*\"vote\" + 0.004*\"email\" + 0.004*\"work\"\n",
      "Topic: 9 Word: 0.007*\"protest\" + 0.005*\"reef\" + 0.004*\"right\" + 0.004*\"action\" + 0.004*\"activist\" + 0.004*\"global\" + 0.004*\"group\" + 0.004*\"crisi\" + 0.004*\"human\" + 0.004*\"polic\"\n"
     ]
    }
   ],
   "source": [
    "for idx, article in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx} Word: {article}\") # index -> weight*word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LDA using TF-IDF\n",
    "- TF-IDF is a measure of how important a word is to a document in a collection or corpus. its a frequency based representation. TF-IDF = TF(term frequency) * IDF(Inverse document frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 55s\n",
      "Wall time: 10min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf,\n",
    "                                             num_topics=10,\n",
    "                                             id2word=dictionary,\n",
    "                                             passes=20,\n",
    "                                             workers=2,\n",
    "                                             iterations=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 2 Word: 0.003*\"emiss\" + 0.003*\"energi\" + 0.003*\"australia\" + 0.002*\"carbon\" + 0.002*\"compani\" + 0.002*\"coal\" + 0.002*\"electr\" + 0.002*\"fuel\" + 0.002*\"plan\" + 0.002*\"power\"\n",
      "Topic: 7 Word: 0.005*\"ridd\" + 0.005*\"bernhardt\" + 0.003*\"accc\" + 0.003*\"sunscreen\" + 0.003*\"beurden\" + 0.003*\"britishvolt\" + 0.002*\"mekong\" + 0.002*\"jenrick\" + 0.002*\"contrail\" + 0.002*\"fishmeal\"\n",
      "Topic: 3 Word: 0.008*\"zink\" + 0.005*\"stork\" + 0.004*\"natal\" + 0.003*\"everglad\" + 0.003*\"formosa\" + 0.003*\"ipb\" + 0.003*\"wolverin\" + 0.003*\"kakadu\" + 0.003*\"hairstreak\" + 0.003*\"yanomami\"\n",
      "Topic: 5 Word: 0.005*\"cácere\" + 0.005*\"whitehaven\" + 0.004*\"mauritius\" + 0.004*\"flemish\" + 0.004*\"virunga\" + 0.004*\"sandpip\" + 0.003*\"interconnector\" + 0.003*\"royc\" + 0.003*\"hitachi\" + 0.002*\"denka\"\n",
      "Topic: 9 Word: 0.037*\"frack\" + 0.020*\"cartoon\" + 0.018*\"moon\" + 0.017*\"shale\" + 0.014*\"cuadrilla\" + 0.007*\"ineo\" + 0.007*\"earthquak\" + 0.006*\"preston\" + 0.006*\"tremor\" + 0.006*\"glencor\"\n",
      "Topic: 6 Word: 0.028*\"protest\" + 0.017*\"polic\" + 0.015*\"amazon\" + 0.014*\"rebellion\" + 0.012*\"brazil\" + 0.012*\"arrest\" + 0.011*\"activist\" + 0.010*\"thunberg\" + 0.010*\"bolsonaro\" + 0.009*\"indigen\"\n",
      "Topic: 4 Word: 0.003*\"speci\" + 0.002*\"fish\" + 0.002*\"bird\" + 0.002*\"tree\" + 0.002*\"anim\" + 0.002*\"park\" + 0.002*\"whale\" + 0.002*\"wildlif\" + 0.002*\"water\" + 0.002*\"photograph\"\n",
      "Topic: 8 Word: 0.009*\"flood\" + 0.007*\"temperatur\" + 0.006*\"water\" + 0.005*\"pollut\" + 0.005*\"weather\" + 0.004*\"rain\" + 0.004*\"heat\" + 0.004*\"record\" + 0.004*\"heatwav\" + 0.004*\"drought\"\n",
      "Topic: 1 Word: 0.005*\"bird\" + 0.005*\"flower\" + 0.004*\"tree\" + 0.004*\"wood\" + 0.004*\"wing\" + 0.004*\"butterfli\" + 0.003*\"diari\" + 0.003*\"winter\" + 0.003*\"spring\" + 0.003*\"white\"\n",
      "Topic: 0 Word: 0.027*\"plastic\" + 0.014*\"recycl\" + 0.012*\"wast\" + 0.012*\"food\" + 0.011*\"meat\" + 0.008*\"farm\" + 0.006*\"product\" + 0.006*\"bottl\" + 0.006*\"farmer\" + 0.005*\"bag\"\n"
     ]
    }
   ],
   "source": [
    "for idx, article in lda_model_tfidf.print_topics(0):\n",
    "    print(f\"Topic: {idx} Word: {article}\") # index -> weight*word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_doc = \"\"\"\n",
    "    Now, emerging evidence suggests reindeer may play a fundamental role in helping to preserve this entire ecosystem, including the snow cover, the open forest with its low-growing berry bushes, mosses and lichen an organism formed by a close association of fungi and algae and even the cold winter climate.\n",
    "\"\"\"\n",
    "\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6993319392204285\t \n",
      "Topic: 0.003*\"speci\" + 0.002*\"fish\" + 0.002*\"bird\" + 0.002*\"tree\" + 0.002*\"anim\" + 0.002*\"park\" + 0.002*\"whale\" + 0.002*\"wildlif\" + 0.002*\"water\" + 0.002*\"photograph\"\n",
      "\n",
      "Score: 0.2730673849582672\t \n",
      "Topic: 0.005*\"bird\" + 0.005*\"flower\" + 0.004*\"tree\" + 0.004*\"wood\" + 0.004*\"wing\" + 0.004*\"butterfli\" + 0.003*\"diari\" + 0.003*\"winter\" + 0.003*\"spring\" + 0.003*\"white\"\n"
     ]
    }
   ],
   "source": [
    "# LDA TF-IDF\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6745323538780212\t \n",
      "Topic: 0.005*\"tree\" + 0.005*\"photograph\" + 0.005*\"bird\" + 0.004*\"plant\" + 0.004*\"look\" + 0.003*\"natur\" + 0.003*\"leav\" + 0.003*\"work\" + 0.003*\"live\" + 0.003*\"know\"\n",
      "\n",
      "Score: 0.29786381125450134\t \n",
      "Topic: 0.011*\"fish\" + 0.010*\"ocean\" + 0.009*\"research\" + 0.008*\"whale\" + 0.007*\"studi\" + 0.007*\"water\" + 0.006*\"scientist\" + 0.005*\"univers\" + 0.005*\"marin\" + 0.004*\"temperatur\"\n"
     ]
    }
   ],
   "source": [
    "# LDA Bag Of Words\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
